import asyncio
import json
from pathlib import Path
import time

import aiofiles
from gsuid_core.logger import logger
import httpx

# GitHub Raw é•œåƒæºåˆ—è¡¨ (å¯æ‰©å±•)
GITHUB_MIRRORS = [
    ("[GitHub Raw]", "https://raw.githubusercontent.com"),  # åŒ…å«ç›´è¿
    # ("[GitHub Mirror CN]", "https://raw.gitmirror.com"),
    ("[GitHub Mirror CN-hub]", "https://hub.gitmirror.com/raw.githubusercontent.com"),
    ("[GitHub Mirror j cdn]", "https://cdn.jsdelivr.net/gh"),
    ("[GitHub Mirror j fastly]", "https://fastly.jsdelivr.net/gh"),
    ("[GitHub Mirror j gcore]", "https://gcore.jsdelivr.net/gh"),
    ("[GitHub Mirror ghproxy]", "https://gh-proxy.org/https://raw.githubusercontent.com"),
]

# ä»“åº“ä¿¡æ¯ (å¯é…ç½®)
GITHUB_REPO_OWNER = "MoonShadow1976"
GITHUB_REPO_NAME = "WutheringWaves_OverSea_StaticAssets"
GITHUB_BRANCH = "main"

# JSONç´¢å¼•è·¯å¾„
INDEX_PATHS = {
    "resource": "data/resource.json",  # é¡¶å±‚ç´¢å¼•
    # å­ç›®å½•ç´¢å¼•: data/resource/xxx.json
}

# ä¸‹è½½é…ç½®
DOWNLOAD_CONFIG = {
    "max_concurrent": 20,  # æœ€å¤§å¹¶å‘æ•°
    "batch_sizes": {
        "small": 10,  # å°æ–‡ä»¶ï¼ˆ<1MBï¼‰
        "medium": 5,  # ä¸­ç­‰æ–‡ä»¶ï¼ˆ1MB-10MBï¼‰
        "large": 2,  # å¤§æ–‡ä»¶ï¼ˆ>10MBï¼‰
    },
    "retry_times": 3,
    "timeout": 30.0,
}


def mirror_head_to_access_url(url: str) -> str:
    """å°†é•œåƒæºURLè½¬æ¢ä¸ºè®¿é—®èµ„æºçš„URLæ ¼å¼"""
    if "jsdelivr.net" in url:
        return f"{url.rstrip('/')}/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}@{GITHUB_BRANCH}"
    else:
        return f"{url.rstrip('/')}/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}"


async def test_mirror_speed(tag: str, base_url: str) -> tuple[str, str, float, dict | None]:
    """æµ‹è¯•å•ä¸ªGitHubé•œåƒæºé€Ÿåº¦"""
    url = mirror_head_to_access_url(base_url) + f"/{INDEX_PATHS['resource']}"

    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            start_time = time.time()
            response = await client.get(url, timeout=10.0)
            elapsed_time = time.time() - start_time

            if response.status_code == 200:
                logger.debug(f"âŒ› [æµ‹é€Ÿ] {tag} {url} å»¶æ—¶: {elapsed_time:.2f}s")
                try:
                    data = json.loads(response.text)
                    if "last_updated" in data:
                        return tag, base_url, elapsed_time, data
                    else:
                        logger.warning(f"âš ï¸ {tag} {url} JSONæ ¼å¼é”™è¯¯: ç¼ºå°‘last_updated")
                        return tag, base_url, elapsed_time, None
                except json.JSONDecodeError:
                    logger.warning(f"âš ï¸ {tag} {url} JSONè§£æå¤±è´¥")
                    return tag, base_url, elapsed_time, None
            else:
                logger.warning(f"âš ï¸ {tag} {url} æµ‹è¯•æ–‡ä»¶çŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            logger.warning(f"âš ï¸ {tag} {url} è¿æ¥é”™è¯¯: {str(e)[:50]}...")

    return tag, base_url, float("inf"), None


async def check_speed():
    """æµ‹é€Ÿé€‰æ‹©æœ€å¿«çš„GitHubé•œåƒæºï¼Œæ¯”è¾ƒèµ„æºæ–°é²œåº¦"""
    logger.info("[WWèµ„æºä¸‹è½½]æµ‹é€Ÿä¸­...")

    tasks = []
    for tag, base_url in GITHUB_MIRRORS:
        tasks.append(asyncio.create_task(test_mirror_speed(tag, base_url)))

    results = await asyncio.gather(*tasks, return_exceptions=True)

    available_sources = []

    for result in results:
        if isinstance(result, (Exception, BaseException)):
            continue
        tag, base_url, elapsed, json_data = result

        if elapsed < float("inf"):
            # è§£ææ›´æ–°æ—¶é—´å­—ç¬¦ä¸²ä¸ºæ—¶é—´æˆ³ (UTC)
            last_updated_str = json_data.get("last_updated", "1970-01-01T00:00:00Z") if json_data else "1970-01-01T00:00:00Z"
            try:
                last_updated_timestamp = time.mktime(time.strptime(last_updated_str, "%Y-%m-%dT%H:%M:%SZ"))
            except Exception:
                last_updated_timestamp = 0

            source_info = {
                "tag": tag,
                "url": base_url.rstrip("/"),
                "time": elapsed,
                "json": json_data,
                "last_updated_timestamp": last_updated_timestamp,  # æ—¶é—´æˆ³ç”¨äºæ’åº
            }
            available_sources.append(source_info)

    if not available_sources:
        logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„é•œåƒæºï¼Œç›´æ¥ä½¿ç”¨GitHub Raw")
        global_url = mirror_head_to_access_url("https://raw.githubusercontent.com")
        global_tag = "[GitHub Raw]"
        return global_tag, global_url

    # æŒ‰æ›´æ–°æ—¶é—´æˆ³é™åºï¼ˆè¶Šæ–°è¶Šå¥½ï¼‰ï¼Œç„¶åæŒ‰å»¶æ—¶å‡åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    available_sources.sort(key=lambda s: (-s["last_updated_timestamp"], s["time"]))

    # è·å–æœ€ä½³çš„æ›´æ–°æ—¶é—´å’Œå¯¹åº”çš„æº
    best_update_timestamp = available_sources[0]["last_updated_timestamp"]
    best_sources = [s for s in available_sources if s["last_updated_timestamp"] == best_update_timestamp]

    if len(best_sources) > 1:
        logger.info(f"ğŸ” æœ‰{len(best_sources)}ä¸ªæºå…·æœ‰ç›¸åŒçš„æœ€æ–°æ›´æ–°æ—¶é—´")
        # åœ¨è¿™äº›å…·æœ‰ç›¸åŒæ›´æ–°æ—¶é—´çš„æºä¸­é€‰æ‹©æœ€å¿«çš„
        best_sources.sort(key=lambda x: x["time"])
        selected_source = best_sources[0]
        logger.info(f"âš¡ åœ¨è¿™äº›æºä¸­é€‰æ‹©æœ€å¿«çš„: {selected_source['tag']} ({selected_source['time']:.2f}s)")
    else:
        selected_source = available_sources[0]
        logger.info(f"ğŸ“… é€‰æ‹©å”¯ä¸€ä¸€ä¸ªèµ„æºæœ€æ–°çš„æº: {selected_source['tag']}")

    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæœ€ä½³æºä¸æ˜¯GitHub Rawä½†æ›´æ–°æ—¶é—´è½åäºGitHub Rawï¼Œæ˜¾ç¤ºè­¦å‘Š
    raw_source = next((s for s in available_sources if s["tag"] == "[GitHub Raw]"), None)
    if raw_source:
        if (
            selected_source["tag"] != "[GitHub Raw]"
            and selected_source["last_updated_timestamp"] < raw_source["last_updated_timestamp"]
        ):
            logger.warning(f"âš ï¸ é€‰æ‹©çš„é•œåƒç«™({selected_source['tag']})èµ„æºæ¯”GitHub Rawæ—§ï¼Œæ”¹ä¸ºä½¿ç”¨GitHub Raw")

            selected_source = raw_source
    else:
        logger.info(f"GitHub Rawä¸å¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨é•œåƒç«™{selected_source['tag']}")

    global_url = mirror_head_to_access_url(selected_source["url"])
    global_tag = selected_source["tag"]
    logger.info(f"ğŸš€ æœ€ç»ˆé€‰æ‹©: {global_tag} {global_url}")

    return global_tag, global_url


async def fetch_json_index(client: httpx.AsyncClient, base_url: str, json_path: str) -> dict | None:
    """è·å–å¹¶è§£æJSONç´¢å¼•æ–‡ä»¶"""
    url = f"{base_url.rstrip('/')}/{json_path}"
    try:
        response = await client.get(url, timeout=30.0)
        if response.status_code == 200:
            return json.loads(response.text)
        else:
            logger.warning(f"è·å–JSONç´¢å¼•å¤±è´¥ {url}: HTTP {response.status_code}")
    except Exception as e:
        logger.warning(f"è·å–JSONç´¢å¼•å¤±è´¥ {url}: {e}")
    return None


async def download(
    url: str, path: Path, name: str, client: httpx.AsyncClient, tag: str = "", max_retries: int = 3
) -> tuple[bool, str]:
    """
    ä¸‹è½½æ–‡ä»¶
    è¿”å›: (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯/ç©ºå­—ç¬¦ä¸²)
    """
    for attempt in range(max_retries):
        try:
            logger.debug(f"{tag} å¼€å§‹ä¸‹è½½ {name} (å°è¯• {attempt + 1}/{max_retries})...")

            response = await client.get(url, follow_redirects=True)

            if response.status_code == 200:
                content = response.content
                path.mkdir(parents=True, exist_ok=True)

                async with aiofiles.open(path / name, "wb") as f:
                    await f.write(content)

                logger.debug(f"{tag} {name} ä¸‹è½½å®Œæˆï¼")
                return True, ""
            else:
                logger.warning(f"{tag} {name} ä¸‹è½½å¤±è´¥ï¼HTTP {response.status_code}")
                return False, f"HTTP {response.status_code}"

        except Exception as e:
            logger.error(f"{tag} {name} ä¸‹è½½å‡ºé”™: {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(1)

    logger.warning(f"{tag} {name} ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
    return False, "ä¸‹è½½å¤±è´¥ï¼Œé‡è¯•æ¬¡æ•°ç”¨å°½"


async def download_with_json_index(
    base_url: str, tag: str, endpoint: str, local_path: Path, client: httpx.AsyncClient, plugin_name: str
) -> tuple[int, int, int, list[str]]:
    """
    ä½¿ç”¨JSONç´¢å¼•ä¸‹è½½å•ä¸ªç›®å½•çš„èµ„æº
    è¿”å›: (æ€»æ–‡ä»¶æ•°, å·²å­˜åœ¨æ–‡ä»¶æ•°, ä¸‹è½½æˆåŠŸæ•°, å¤±è´¥æ–‡ä»¶åˆ—è¡¨)
    """
    dir_name = endpoint.split("/")[-1] if "/" in endpoint else endpoint

    dir_json_path = f"data/resource/{dir_name}.json"
    dir_json = await fetch_json_index(client, base_url, dir_json_path)

    if not dir_json:
        logger.warning(f"{plugin_name} {tag} {endpoint} æ— æ³•è·å–JSONç´¢å¼•: {dir_json_path}")
        return 0, 0, 0, []

    files = dir_json.get("files", [])
    total_files = len(files)
    exist_files = 0
    need_download_files = 0

    logger.info(f"{plugin_name} {tag} ç›®å½• {endpoint} ä¸­æœ‰ {total_files} ä¸ªæ–‡ä»¶å¾…æ£€æŸ¥")

    # åˆ†ç±»æ–‡ä»¶ï¼šæŒ‰å¤§å°åˆ†ç»„
    small_files = []  # <1MB
    medium_files = []  # 1MB-10MB
    large_files = []  # >10MB

    for file_info in files:
        file_relative_path = file_info["path"]
        remote_size = file_info.get("size", 0)

        if file_relative_path.startswith(dir_name + "/"):
            local_relative_path = file_relative_path[len(dir_name) + 1 :]
        else:
            local_relative_path = file_info["name"]

        local_file_path = local_path / local_relative_path

        file_exists = local_file_path.exists()

        if file_exists:
            exist_files += 1
            local_size = local_file_path.stat().st_size

            if local_size == remote_size:
                continue
            else:
                logger.info(
                    f"{plugin_name} {tag}ğŸ”„ æ–‡ä»¶å¤§å°ä¸ä¸€è‡´: {file_relative_path} (æœ¬åœ°: {local_size}, è¿œç¨‹: {remote_size})"
                )
        else:
            local_file_path.parent.mkdir(parents=True, exist_ok=True)

        need_download_files += 1

        file_url = f"{base_url.rstrip('/')}/data/resource/{file_relative_path}"

        # æŒ‰æ–‡ä»¶å¤§å°åˆ†ç±»
        if remote_size < 1024 * 1024:  # <1MB
            small_files.append((file_url, local_file_path.parent, local_file_path.name, file_relative_path))
        elif remote_size < 10 * 1024 * 1024:  # 1MB-10MB
            medium_files.append((file_url, local_file_path.parent, local_file_path.name, file_relative_path))
        else:  # >10MB
            large_files.append((file_url, local_file_path.parent, local_file_path.name, file_relative_path))

    logger.debug(
        f"{tag} ç›®å½• {endpoint} éœ€è¦ä¸‹è½½ {need_download_files} ä¸ªæ–‡ä»¶ (å°: {len(small_files)}, ä¸­: {len(medium_files)}, å¤§: {len(large_files)})"
    )

    success_count = 0
    failed_files = []

    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(DOWNLOAD_CONFIG["max_concurrent"])

    async def download_with_semaphore(file_url, path, name, file_path):
        async with semaphore:
            success, error_msg = await download(file_url, path, name, client, tag, DOWNLOAD_CONFIG["retry_times"])
            return success, error_msg, file_path

    # åˆ†æ‰¹ä¸‹è½½ä¸åŒç±»å‹æ–‡ä»¶
    download_groups = [
        (small_files, DOWNLOAD_CONFIG["batch_sizes"]["small"], "å°æ–‡ä»¶"),
        (medium_files, DOWNLOAD_CONFIG["batch_sizes"]["medium"], "ä¸­ç­‰æ–‡ä»¶"),
        (large_files, DOWNLOAD_CONFIG["batch_sizes"]["large"], "å¤§æ–‡ä»¶"),
    ]

    for file_list, batch_size, file_type in download_groups:
        if not file_list:
            continue

        logger.debug(f"{tag} å¼€å§‹ä¸‹è½½{file_type}ï¼Œæ•°é‡: {len(file_list)}ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")

        for i in range(0, len(file_list), batch_size):
            batch = file_list[i : i + batch_size]
            batch_tasks = []

            for file_url, path, name, file_path in batch:
                task = asyncio.create_task(download_with_semaphore(file_url, path, name, file_path))
                batch_tasks.append(task)

            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*batch_tasks)

            for success, error_msg, file_path in batch_results:
                if success:
                    success_count += 1
                else:
                    failed_files.append(f"{file_path}: {error_msg}")

            # å°æ‰¹æ¬¡ä¹‹é—´çŸ­æš‚æš‚åœï¼Œé¿å…è¯·æ±‚è¿‡çŒ›
            if i + batch_size < len(file_list):
                await asyncio.sleep(0.1)

    logger.info(
        f"{tag} ç›®å½• {endpoint} æ£€æŸ¥å®Œæˆ: æ€»æ•°={total_files}, å·²å­˜åœ¨={exist_files}, ä¸‹è½½æˆåŠŸ={success_count}, å¤±è´¥={len(failed_files)}"
    )

    return total_files, exist_files, success_count, failed_files


async def download_all_file(
    plugin_name: str,
    EPATH_MAP: dict[str, Path],
    URL: str | None = None,
    TAG: str | None = None,
    max_concurrent: int | None = None,
) -> str:
    """
    ä¸»ä¸‹è½½å‡½æ•° - æ”¯æŒåŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
    è¿”å›: ç®€åŒ–çš„ä¸‹è½½ç»“æœå­—ç¬¦ä¸²
    """
    # æ›´æ–°é…ç½®
    if max_concurrent:
        DOWNLOAD_CONFIG["max_concurrent"] = max_concurrent

    if URL:
        TAG, BASE_URL = TAG or "[Unknown]", URL
    else:
        TAG, BASE_URL = await check_speed()
        if not BASE_URL:
            return "âŒ æ— æ³•è·å–å¯ç”¨çš„èµ„æºç«™"

    logger.info(f"ğŸ”— {plugin_name} ä½¿ç”¨èµ„æºç«™: {TAG}ï¼Œæœ€å¤§å¹¶å‘æ•°: {DOWNLOAD_CONFIG['max_concurrent']}")

    async with httpx.AsyncClient(
        timeout=httpx.Timeout(DOWNLOAD_CONFIG["timeout"]),
        follow_redirects=True,
        limits=httpx.Limits(max_connections=DOWNLOAD_CONFIG["max_concurrent"] * 2, max_keepalive_connections=10),
    ) as client:
        resource_json = await fetch_json_index(client, BASE_URL, INDEX_PATHS["resource"])
        if not resource_json:
            return "âŒ æ— æ³•è·å–é¡¶å±‚èµ„æºç´¢å¼•ï¼Œå¯èƒ½ç´¢å¼•æ–‡ä»¶æœªç”Ÿæˆ"

        available_dirs = resource_json.get("directories", [])

        processed_dirs = 0
        total_files = 0
        total_exist = 0
        total_success = 0
        total_fail = 0
        failed_dirs_info = []

        for endpoint, local_path in EPATH_MAP.items():
            dir_name = endpoint.split("/")[-1] if "/" in endpoint else endpoint

            if dir_name not in available_dirs:
                logger.warning(f"âš ï¸ ç›®å½• {dir_name} ä¸åœ¨ {plugin_name} èµ„æºç´¢å¼•ä¸­ï¼Œè·³è¿‡")
                failed_dirs_info.append(f"âš ï¸ {dir_name}: ç›®å½•ä¸åœ¨ç´¢å¼•ä¸­")
                continue

            local_path.mkdir(parents=True, exist_ok=True)

            dir_total, dir_exist, dir_success, failed_files = await download_with_json_index(
                BASE_URL, TAG, endpoint, local_path, client, plugin_name
            )

            total_files += dir_total
            total_exist += dir_exist
            total_success += dir_success
            total_fail += len(failed_files)

            if failed_files:
                failed_dirs_info.append(f"âŒ {dir_name}: {dir_success}æˆåŠŸ, {len(failed_files)}å¤±è´¥")
            else:
                failed_dirs_info.append(f"âœ… {dir_name}: {dir_success}ä¸ªæ–‡ä»¶ä¸‹è½½å®Œæˆ")

            processed_dirs += 1

        # ç”Ÿæˆç»“æœå­—ç¬¦ä¸²
        total_need_download = total_files - total_exist
        failed_items = [info for info in failed_dirs_info if "âŒ" in info]
        max_display = 5

        if total_fail == 0:
            if total_need_download == 0:
                return f"âœ… æ‰€æœ‰{processed_dirs}ä¸ªç›®å½•å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€ä¸‹è½½"
            else:
                return f"âœ… æ‰€æœ‰{processed_dirs}ä¸ªç›®å½•ä¸‹è½½å®Œæˆï¼Œ{total_success}ä¸ªæ–‡ä»¶ä¸‹è½½æˆåŠŸ"
        else:
            result_lines = [f"âŒ {processed_dirs}ä¸ªç›®å½•ï¼Œ{total_success}æˆåŠŸ/{total_fail}å¤±è´¥"]

            if failed_items:
                if len(failed_items) > max_display:
                    result_lines.extend(failed_items[:max_display])
                    result_lines.append(f"... è¿˜æœ‰ {len(failed_items) - max_display} ä¸ªç›®å½•å¤±è´¥æœªæ˜¾ç¤º")
                else:
                    result_lines.extend(failed_items)

            return "\n".join(result_lines)
