import time
import asyncio
import json
from pathlib import Path
from typing import Dict, Optional, Tuple

import httpx
from gsuid_core.logger import logger

from gsuid_core.utils.download_resource.download_file import download

# å…¨å±€ç¼“å­˜
global_tag, global_url = '', ''
NOW_SPEED_TEST = False

# GitHub Raw é•œåƒæºåˆ—è¡¨ (å¯æ‰©å±•)
GITHUB_MIRRORS = [
    # ("[GitHub Raw]", "https://raw.githubusercontent.com"),
    # ("[GitHub Mirror CN]", "https://raw.gitmirror.com"),
    ("[GitHub Mirror CN-hub]", "https://hub.gitmirror.com/raw.githubusercontent.com"),
    ("[GitHub Mirror j cdn]", "https://cdn.jsdelivr.net/gh"),
    ("[GitHub Mirror j fastly]", "https://fastly.jsdelivr.net/gh"),
    ("[GitHub Mirror j gcore]", "https://gcore.jsdelivr.net/gh"),
    ("[GitHub Mirror fastgit]", "https://raw.fastgit.org"),
    ("[GitHub Mirror ghproxy]", "https://ghproxy.com/https://raw.githubusercontent.com"),
]

# ä»“åº“ä¿¡æ¯ (å¯é…ç½®)
GITHUB_REPO_OWNER = "MoonShadow1976"
GITHUB_REPO_NAME = "WutheringWaves_OverSea_StaticAssets"
GITHUB_BRANCH = "main"

# JSONç´¢å¼•è·¯å¾„ (æ‰å¹³åŒ–ç»“æ„)
INDEX_PATHS = {
    "resource": "data/resource.json",  # é¡¶å±‚ç´¢å¼•
    # å­ç›®å½•ç´¢å¼•: data/resource/xxx.json
}


async def test_mirror_speed(tag: str, base_url: str) -> Tuple[str, str, float]:
    """æµ‹è¯•å•ä¸ªGitHubé•œåƒæºé€Ÿåº¦"""
    test_file = f"{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}/{INDEX_PATHS['resource']}"
    url = f"{base_url.rstrip('/')}/{test_file}"
    
    async with httpx.AsyncClient() as client:
        try:
            start_time = time.time()
            response = await client.get(url, timeout=10.0)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                logger.debug(f'âŒ› [æµ‹é€Ÿ] {tag} {base_url} å»¶æ—¶: {elapsed_time:.2f}s')
                return tag, base_url, elapsed_time
            else:
                logger.info(f'âš  {tag} {base_url} æµ‹è¯•æ–‡ä»¶çŠ¶æ€ç : {response.status_code}')
        except Exception as e:
            logger.info(f'âš  {tag} {base_url} è¿æ¥é”™è¯¯: {str(e)[:50]}...')
    
    return tag, base_url, float('inf')


async def check_speed():
    """æµ‹é€Ÿé€‰æ‹©æœ€å¿«çš„GitHubé•œåƒæº (ä¼˜å…ˆä½¿ç”¨GitHub Rawï¼Œå¦‚æœå¯ç”¨)"""
    global global_tag, global_url, NOW_SPEED_TEST
    
    if (not global_tag or not global_url) and not NOW_SPEED_TEST:
        NOW_SPEED_TEST = True
        logger.info('[WWCoreèµ„æºä¸‹è½½]æµ‹é€Ÿä¸­...')
        
        # ç¬¬ä¸€æ­¥ï¼šä¼˜å…ˆæµ‹è¯•GitHub Rawï¼ˆåŸç«™ï¼‰
        raw_tag = "[GitHub Raw]"
        raw_url = "https://raw.githubusercontent.com"
        
        logger.info(f'ğŸ” ä¼˜å…ˆæµ‹è¯•åŸç«™: {raw_tag}')
        raw_tag_result, raw_url_result, raw_time = await test_mirror_speed(raw_tag, raw_url)
        
        # å¦‚æœGitHub Rawå¯ä»¥è®¿é—®ä¸”é€Ÿåº¦å¿«äº5ç§’ï¼Œç›´æ¥ä½¿ç”¨
        if raw_time < 5.0:
            logger.info('âœ… GitHub Rawå¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨åŸç«™')
            global_tag = raw_tag_result
            global_url = f"{raw_url_result.rstrip('/')}/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}"
            NOW_SPEED_TEST = False
            logger.info(f"ğŸš€ ä½¿ç”¨èµ„æºç«™: {global_tag} {global_url}")
            return global_tag, global_url
        
        # ç¬¬äºŒæ­¥ï¼šå¦‚æœGitHub Rawä¸å¯ç”¨ï¼Œæµ‹è¯•æ‰€æœ‰é•œåƒæº
        logger.info('âŒ GitHub Rawä¸å¯ç”¨ï¼Œå¼€å§‹æµ‹è¯•é•œåƒæº...')
        tasks = []
        for tag, base_url in GITHUB_MIRRORS:
            tasks.append(asyncio.create_task(test_mirror_speed(tag, base_url)))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        fastest_tag = ''
        fastest_url = ''
        fastest_time = float('inf')
        
        for result in results:
            if isinstance(result, (Exception, BaseException)):
                continue
            tag, base_url, elapsed = result
            if elapsed < fastest_time:
                fastest_time = elapsed
                fastest_tag = tag
                fastest_url = base_url
        
        # æ„å»ºå®Œæ•´çš„èµ„æºç«™URL
        if fastest_url:
            global_url = f"{fastest_url.rstrip('/')}/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}"
            global_tag = fastest_tag
            logger.info(f"ğŸš€ æœ€å¿«é•œåƒæº: {global_tag} {global_url}")
        else:
            # å¦‚æœæ‰€æœ‰é•œåƒéƒ½å¤±è´¥ï¼Œä»ç„¶ä½¿ç”¨åŸç«™ä½œä¸ºåå¤‡ï¼ˆå³ä½¿å¯èƒ½ä¸å¯ç”¨ï¼‰
            global_url = f"https://raw.githubusercontent.com/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}"
            global_tag = "[GitHub Raw]"
            logger.warning(f"âš ï¸ æ‰€æœ‰é•œåƒæµ‹é€Ÿå¤±è´¥ï¼Œä½¿ç”¨åŸç«™ï¼ˆå¯èƒ½ä¸å¯ç”¨ï¼‰: {global_tag}")
        
        NOW_SPEED_TEST = False
        return global_tag, global_url
    
    if NOW_SPEED_TEST:
        while NOW_SPEED_TEST:
            await asyncio.sleep(0.5)
    
    return global_tag, global_url


async def fetch_json_index(client: httpx.AsyncClient, base_url: str, json_path: str) -> Optional[Dict]:
    """è·å–å¹¶è§£æJSONç´¢å¼•æ–‡ä»¶"""
    url = f"{base_url.rstrip('/')}/{json_path}"
    try:
        response = await client.get(url, timeout=30.0)
        if response.status_code == 200:
            return json.loads(response.text)
    except Exception as e:
        logger.warning(f"è·å–JSONç´¢å¼•å¤±è´¥ {url}: {e}")
    return None


async def download_with_json_index(
    base_url: str,
    tag: str,
    endpoint: str,
    local_path: Path,
    client: httpx.AsyncClient,
    plugin_name: str
):
    """ä½¿ç”¨JSONç´¢å¼•ä¸‹è½½å•ä¸ªç›®å½•çš„èµ„æº"""
    # ä»endpointæå–ç›®å½•å
    dir_name = endpoint.split('/')[-1] if '/' in endpoint else endpoint
    
    # è·å–ç›®å½•çš„JSONç´¢å¼•
    dir_json_path = f"data/resource/{dir_name}.json"
    dir_json = await fetch_json_index(client, base_url, dir_json_path)
    
    if not dir_json:
        logger.warning(f'{plugin_name} {tag} {endpoint} æ— æ³•è·å–JSONç´¢å¼•: {dir_json_path}')
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    files = dir_json.get("files", [])
    total_files = len(files)
    exist_files = 0
    need_download_files = 0
    logger.debug(f'{plugin_name} {tag} ç›®å½• {endpoint} ä¸­æœ‰ {total_files} ä¸ªæ–‡ä»¶å¾…æ£€æŸ¥')
    
    # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
    download_tasks = []
    size_checked = 0
    batch_size_limit = 1500000  # 1.5MB æ‰¹æ¬¡é™åˆ¶
    batch_num = 0  # æ‰¹æ¬¡ç¼–å·ï¼Œç”¨äºæ—¥å¿—
    
    for idx, file_info in enumerate(files, 1):
        file_relative_path = file_info["path"]
        remote_size = file_info.get("size", 0)
        
        # æ„å»ºæœ¬åœ°è·¯å¾„
        if file_relative_path.startswith(dir_name + "/"):
            local_relative_path = file_relative_path[len(dir_name)+1:]
        else:
            local_relative_path = file_info["name"]
        
        local_file_path = local_path / local_relative_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¤§å°ä¸€è‡´
        file_exists = local_file_path.exists()
        
        if file_exists:
            exist_files += 1
            local_size = local_file_path.stat().st_size
            
            if local_size == remote_size:
                logger.debug(f'{tag}âœ… æ–‡ä»¶å·²å­˜åœ¨: {file_relative_path}')
                continue  # æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°ä¸€è‡´ï¼Œè·³è¿‡ä¸‹è½½
            else:
                logger.info(f'{plugin_name} {tag}ğŸ”„ æ–‡ä»¶å¤§å°ä¸ä¸€è‡´: {file_relative_path} (æœ¬åœ°: {local_size}, è¿œç¨‹: {remote_size})')
        else:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        need_download_files += 1
        size_checked += remote_size
        
        # æ„å»ºä¸‹è½½URL
        file_url = f"{base_url.rstrip('/')}/data/resource/{file_relative_path}"
        
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        task = asyncio.create_task(
            download(file_url, local_file_path.parent, local_file_path.name, client, tag)
        )
        download_tasks.append(task)
        
        # æ‰¹æ¬¡æ§åˆ¶ï¼šè¾¾åˆ°é™åˆ¶æˆ–å¤„ç†å®Œæœ€åä¸€ä¸ªæ–‡ä»¶æ—¶
        if size_checked >= batch_size_limit or idx == total_files:
            batch_num += 1
            
            if len(download_tasks) > 0:
                logger.debug(f'{tag} å¼€å§‹ç¬¬ {batch_num} æ‰¹ä¸‹è½½ï¼Œå…± {len(download_tasks)} ä¸ªæ–‡ä»¶')
                await asyncio.gather(*download_tasks)
            
            # é‡ç½®æ‰¹æ¬¡
            download_tasks.clear()
            size_checked = 0
    
    logger.info(f'{tag} ç›®å½• {endpoint} æ£€æŸ¥å®Œæˆ-> æ€»æ•°: {total_files}, æœ¬åœ°å·²å­˜åœ¨: {exist_files}, éœ€ä¸‹è½½: {need_download_files}')


async def download_all_file(
    plugin_name: str,
    EPATH_MAP: Dict[str, Path],
    URL: Optional[str] = None,
    TAG: Optional[str] = None,
):
    """ä¸»ä¸‹è½½å‡½æ•° (æ¥å£ä¿æŒä¸å˜)"""
    
    # 1. ç¡®å®šèµ„æºç«™URLå’ŒTAG
    if URL:
        TAG, BASE_URL = TAG or '[Unknown]', URL
    else:
        TAG, BASE_URL = await check_speed()
        if not BASE_URL:
            logger.error("âŒ æ— æ³•è·å–å¯ç”¨çš„èµ„æºç«™")
            return
    
    logger.info(f'ğŸ”— {plugin_name} ä½¿ç”¨èµ„æºç«™: {TAG}')
    
    # 2. è·å–é¡¶å±‚èµ„æºç´¢å¼•ï¼ŒéªŒè¯ç›®å½•å­˜åœ¨
    async with httpx.AsyncClient(timeout=httpx.Timeout(200.0)) as client:
        # è·å–é¡¶å±‚ç´¢å¼•
        resource_json = await fetch_json_index(client, BASE_URL, INDEX_PATHS['resource'])
        if not resource_json:
            logger.error(f'âŒ {plugin_name} æ— æ³•è·å–é¡¶å±‚èµ„æºç´¢å¼•ï¼Œå¯èƒ½ç´¢å¼•æ–‡ä»¶æœªç”Ÿæˆ')
            return
        
        available_dirs = resource_json.get('directories', [])
        
        # 3. éå†æ‰€æœ‰endpointè¿›è¡Œä¸‹è½½
        processed_count = 0
        for endpoint, local_path in EPATH_MAP.items():
            # æå–ç›®å½•å
            dir_name = endpoint.split('/')[-1] if '/' in endpoint else endpoint
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦åœ¨ç´¢å¼•ä¸­
            if dir_name not in available_dirs:
                logger.warning(f'âš  ç›®å½• {dir_name} ä¸åœ¨ {plugin_name} èµ„æºç´¢å¼•ä¸­ï¼Œè·³è¿‡')
                continue
            
            # ç¡®ä¿æœ¬åœ°ç›®å½•å­˜åœ¨
            local_path.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½è¯¥ç›®å½•èµ„æº
            await download_with_json_index(
                BASE_URL, TAG, endpoint, local_path, client, plugin_name
            )
            processed_count += 1
        
        # 4. æœ€ç»ˆçŠ¶æ€
        if processed_count == len(EPATH_MAP):
            logger.success(f'ğŸ± [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} æ‰€æœ‰èµ„æºå·²æ˜¯æœ€æ–°!')
        elif processed_count > 0:
            logger.success(f'ğŸ“¦ [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} å·²å®Œæˆ {processed_count}/{len(EPATH_MAP)} ä¸ªç›®å½•')
        else:
            logger.warning(f'âš  [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„èµ„æºç›®å½•')