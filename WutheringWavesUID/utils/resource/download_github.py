import os
import time
import asyncio
import json
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from urllib.parse import urljoin

import httpx
from gsuid_core.logger import logger

from gsuid_core.utils.download_resource.download_file import download

# å…¨å±€ç¼“å­˜
global_tag, global_url = '', ''
NOW_SPEED_TEST = False

# GitHub Raw é•œåƒæºåˆ—è¡¨ (å¯æ‰©å±•)
GITHUB_MIRRORS = [
    ("[GitHub Raw]", "https://raw.githubusercontent.com"),
    ("[GitHub Mirror CN]", "https://raw.gitmirror.com"),
    ("[GitHub Mirror Fast]", "https://ghproxy.com/https://raw.githubusercontent.com"),
    ("[GitHub Mirror]", "https://raw.fastgit.org"),
]

# ä»“åº“ä¿¡æ¯ (å¯é…ç½®)
GITHUB_REPO_OWNER = "MoonShadow1976"
GITHUB_REPO_NAME = "WutheringWaves_OverSea_StaticAssets"
GITHUB_BRANCH = "main"

# JSONç´¢å¼•è·¯å¾„ (æ‰å¹³åŒ–ç»“æ„)
INDEX_PATHS = {
    "resource": "data/resource.json",  # é¡¶å±‚ç´¢å¼•
    # å­ç›®å½•ç´¢å¼•: data/resource/xxx.json
}


async def test_mirror_speed(tag: str, base_url: str) -> Tuple[str, str, float]:
    """æµ‹è¯•å•ä¸ªGitHubé•œåƒæºé€Ÿåº¦"""
    test_file = f"{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}/{INDEX_PATHS['resource']}"
    url = f"{base_url.rstrip('/')}/{test_file}"
    
    async with httpx.AsyncClient() as client:
        try:
            start_time = time.time()
            response = await client.get(url, timeout=10.0)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                logger.debug(f'âŒ› [æµ‹é€Ÿ] {tag} {base_url} å»¶æ—¶: {elapsed_time:.2f}s')
                return tag, base_url, elapsed_time
            else:
                logger.info(f'âš  {tag} {base_url} æµ‹è¯•æ–‡ä»¶çŠ¶æ€ç : {response.status_code}')
        except Exception as e:
            logger.info(f'âš  {tag} {base_url} è¿æ¥é”™è¯¯: {str(e)[:50]}...')
    
    return tag, base_url, float('inf')


async def check_speed():
    """æµ‹é€Ÿé€‰æ‹©æœ€å¿«çš„GitHubé•œåƒæº (ä¿æŒåŸæœ‰æ¥å£)"""
    global global_tag, global_url, NOW_SPEED_TEST
    
    if (not global_tag or not global_url) and not NOW_SPEED_TEST:
        NOW_SPEED_TEST = True
        logger.info('[WWCoreèµ„æºä¸‹è½½]æµ‹é€Ÿä¸­...')
        
        # å¹¶å‘æµ‹è¯•æ‰€æœ‰é•œåƒæº
        tasks = []
        for tag, base_url in GITHUB_MIRRORS:
            tasks.append(asyncio.create_task(test_mirror_speed(tag, base_url)))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        fastest_tag = ''
        fastest_url = ''
        fastest_time = float('inf')
        
        for result in results:
            if isinstance(result, (Exception, BaseException)):
                continue
            tag, base_url, elapsed = result
            if elapsed < fastest_time:
                fastest_time = elapsed
                fastest_tag = tag
                fastest_url = base_url
        
        # æ„å»ºå®Œæ•´çš„èµ„æºç«™URL
        if fastest_url:
            global_url = f"{fastest_url.rstrip('/')}/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}"
            global_tag = fastest_tag
            logger.info(f"ğŸš€ æœ€å¿«èµ„æºç«™: {global_tag} {global_url}")
        else:
            # å¦‚æœæ‰€æœ‰é•œåƒéƒ½å¤±è´¥ï¼Œä½¿ç”¨ä¸»ç«™ä½œä¸ºåå¤‡
            global_url = f"https://raw.githubusercontent.com/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}"
            global_tag = "[GitHub Raw]"
            logger.warning(f"âš  æ‰€æœ‰é•œåƒæµ‹é€Ÿå¤±è´¥ï¼Œä½¿ç”¨ä¸»ç«™: {global_tag}")
        
        NOW_SPEED_TEST = False
        return global_tag, global_url
    
    if NOW_SPEED_TEST:
        while NOW_SPEED_TEST:
            await asyncio.sleep(0.5)
    
    return global_tag, global_url


async def fetch_json_index(client: httpx.AsyncClient, base_url: str, json_path: str) -> Optional[Dict]:
    """è·å–å¹¶è§£æJSONç´¢å¼•æ–‡ä»¶"""
    url = f"{base_url.rstrip('/')}/{json_path}"
    try:
        response = await client.get(url, timeout=30.0)
        if response.status_code == 200:
            return json.loads(response.text)
    except Exception as e:
        logger.warning(f"è·å–JSONç´¢å¼•å¤±è´¥ {url}: {e}")
    return None


async def download_with_json_index(
    base_url: str,
    tag: str,
    endpoint: str,
    local_path: Path,
    client: httpx.AsyncClient,
    plugin_name: str
):
    """ä½¿ç”¨JSONç´¢å¼•ä¸‹è½½å•ä¸ªç›®å½•çš„èµ„æº"""
    
    # ä»endpointæå–ç›®å½•åï¼ˆç”¨äºæŸ¥æ‰¾JSONç´¢å¼•ï¼‰
    # endpointæ ¼å¼: "resource/avatar" -> ç›®å½•å: "avatar"
    dir_name = endpoint.split('/')[-1] if '/' in endpoint else endpoint
    
    # è·å–ç›®å½•çš„JSONç´¢å¼•
    dir_json_path = f"data/resource/{dir_name}.json"
    dir_json = await fetch_json_index(client, base_url, dir_json_path)
    
    if not dir_json:
        logger.warning(f'{tag} {endpoint} æ— æ³•è·å–JSONç´¢å¼•: {dir_json_path}')
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    files = dir_json.get("files", [])
    total_files = len(files)
    exist_files = 0
    need_download_files = 0
    logger.info(f'{tag} ç›®å½• {endpoint} ä¸­æœ‰ {total_files} ä¸ªæ–‡ä»¶å¾…æ£€æŸ¥')
    
    # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
    download_tasks = []
    
    for file_info in files:
        # file_infoä¸­çš„pathæ˜¯ç›¸å¯¹äºdata/resource/çš„å®Œæ•´è·¯å¾„
        # ä¾‹å¦‚: "avatar/1001.png" æˆ– "avatar/special/1003.png"
        file_relative_path = file_info["path"]
        remote_size = file_info.get("size", 0)
        
        # ä»å®Œæ•´çš„ç›¸å¯¹è·¯å¾„ä¸­ç§»é™¤ç›®å½•åå‰ç¼€
        # ä¾‹å¦‚: "avatar/1001.png" -> å»æ‰ "avatar/" -> "1001.png"
        # ä¾‹å¦‚: "avatar/special/1003.png" -> å»æ‰ "avatar/" -> "special/1003.png"
        if file_relative_path.startswith(dir_name + "/"):
            # ç§»é™¤ç›®å½•åå‰ç¼€å’Œåé¢çš„æ–œæ 
            local_relative_path = file_relative_path[len(dir_name)+1:]
        else:
            # å¦‚æœä¸ä»¥ç›®å½•åå¼€å¤´ï¼Œå¯èƒ½æ˜¯å…¶ä»–æƒ…å†µï¼Œä½¿ç”¨æ–‡ä»¶å
            local_relative_path = file_info["name"]
        
        # æ„å»ºæœ¬åœ°å®Œæ•´è·¯å¾„
        local_file_path = local_path / local_relative_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¤§å°åŒ¹é…
        file_exists = local_file_path.exists()
        
        if file_exists:
            exist_files += 1
            local_size = local_file_path.stat().st_size
            
            if local_size == remote_size:
                # æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°ä¸€è‡´ï¼Œè·³è¿‡ä¸‹è½½
                continue
            else:
                # æ–‡ä»¶å­˜åœ¨ä½†å¤§å°ä¸ä¸€è‡´ï¼Œéœ€è¦é‡æ–°ä¸‹è½½
                logger.debug(f'{tag}ğŸ”„ æ–‡ä»¶å¤§å°ä¸ä¸€è‡´: {file_relative_path} (æœ¬åœ°: {local_size}, è¿œç¨‹: {remote_size})')
                need_download_files += 1
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½
            need_download_files += 1
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºä¸‹è½½URL
        file_url = f"{base_url.rstrip('/')}/data/resource/{file_relative_path}"
        
        logger.info(f'{tag} {plugin_name} å¼€å§‹ä¸‹è½½ {file_relative_path}')
        
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        task = asyncio.create_task(
            download(file_url, local_file_path.parent, local_file_path.name, client, tag)
        )
        download_tasks.append(task)
        
        # æ‰¹æ¬¡æ§åˆ¶
        if len(download_tasks) >= 5:  # æ¯æ‰¹5ä¸ªä»»åŠ¡
            await asyncio.gather(*download_tasks)
            download_tasks.clear()
    
    # æ‰§è¡Œå‰©ä½™ä¸‹è½½ä»»åŠ¡
    if download_tasks:
        await asyncio.gather(*download_tasks)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info(f'{tag} ç›®å½• {endpoint} æ£€æŸ¥å®Œæˆ:')
    logger.info(f'  æ€»æ•°: {total_files}, å·²å­˜åœ¨: {exist_files}, éœ€ä¸‹è½½: {need_download_files}')
    
    if need_download_files == 0:
        logger.success(f'{tag} ç›®å½• {endpoint} æ‰€æœ‰æ–‡ä»¶å·²æ˜¯æœ€æ–°!')


async def download_all_file(
    plugin_name: str,
    EPATH_MAP: Dict[str, Path],
    URL: Optional[str] = None,
    TAG: Optional[str] = None,
):
    """ä¸»ä¸‹è½½å‡½æ•° (æ¥å£ä¿æŒä¸å˜)"""
    
    # 1. ç¡®å®šèµ„æºç«™URLå’ŒTAG
    if URL:
        TAG, BASE_URL = TAG or '[Unknown]', URL
    else:
        TAG, BASE_URL = await check_speed()
        if not BASE_URL:
            logger.error("âŒ æ— æ³•è·å–å¯ç”¨çš„èµ„æºç«™")
            return
    
    logger.info(f'ğŸ”— ä½¿ç”¨èµ„æºç«™: {TAG}')
    
    # 2. è·å–é¡¶å±‚èµ„æºç´¢å¼•ï¼ŒéªŒè¯ç›®å½•å­˜åœ¨
    async with httpx.AsyncClient(timeout=httpx.Timeout(200.0)) as client:
        # è·å–é¡¶å±‚ç´¢å¼•
        resource_json = await fetch_json_index(client, BASE_URL, INDEX_PATHS['resource'])
        if not resource_json:
            logger.error('âŒ æ— æ³•è·å–é¡¶å±‚èµ„æºç´¢å¼•ï¼Œå¯èƒ½ç´¢å¼•æ–‡ä»¶æœªç”Ÿæˆ')
            return
        
        available_dirs = resource_json.get('directories', [])
        
        # 3. éå†æ‰€æœ‰endpointè¿›è¡Œä¸‹è½½
        processed_count = 0
        for endpoint, local_path in EPATH_MAP.items():
            # æå–ç›®å½•å
            dir_name = endpoint.split('/')[-1] if '/' in endpoint else endpoint
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦åœ¨ç´¢å¼•ä¸­
            if dir_name not in available_dirs:
                logger.warning(f'âš  ç›®å½• {dir_name} ä¸åœ¨èµ„æºç´¢å¼•ä¸­ï¼Œè·³è¿‡')
                continue
            
            # ç¡®ä¿æœ¬åœ°ç›®å½•å­˜åœ¨
            local_path.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½è¯¥ç›®å½•èµ„æº
            await download_with_json_index(
                BASE_URL, TAG, endpoint, local_path, client, plugin_name
            )
            processed_count += 1
        
        # 4. æœ€ç»ˆçŠ¶æ€
        if processed_count == len(EPATH_MAP):
            logger.success(f'ğŸ± [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} æ‰€æœ‰èµ„æºå·²æ˜¯æœ€æ–°!')
        elif processed_count > 0:
            logger.info(f'ğŸ“¦ [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} å·²å®Œæˆ {processed_count}/{len(EPATH_MAP)} ä¸ªç›®å½•')
        else:
            logger.warning(f'âš  [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„èµ„æºç›®å½•')