import time
import asyncio
import json
from pathlib import Path
from typing import Dict, Optional, Tuple

import httpx
from gsuid_core.logger import logger

from gsuid_core.utils.download_resource.download_file import download

# å…¨å±€ç¼“å­˜
global_tag, global_url = '', ''
NOW_SPEED_TEST = False

# GitHub Raw é•œåƒæºåˆ—è¡¨ (å¯æ‰©å±•)
GITHUB_MIRRORS = [
    ("[GitHub Raw]", "https://raw.githubusercontent.com"),  # åŒ…å«ç›´è¿
    # ("[GitHub Mirror CN]", "https://raw.gitmirror.com"),
    ("[GitHub Mirror CN-hub]", "https://hub.gitmirror.com/raw.githubusercontent.com"),
    ("[GitHub Mirror j cdn]", "https://cdn.jsdelivr.net/gh"),
    ("[GitHub Mirror j fastly]", "https://fastly.jsdelivr.net/gh"),
    ("[GitHub Mirror j gcore]", "https://gcore.jsdelivr.net/gh"),
    ("[GitHub Mirror fastgit]", "https://raw.fastgit.org"),
    ("[GitHub Mirror ghproxy]", "https://ghproxy.com/https://raw.githubusercontent.com"),
]

# ä»“åº“ä¿¡æ¯ (å¯é…ç½®)
GITHUB_REPO_OWNER = "MoonShadow1976"
GITHUB_REPO_NAME = "WutheringWaves_OverSea_StaticAssets"
GITHUB_BRANCH = "main"

# JSONç´¢å¼•è·¯å¾„ (æ‰å¹³åŒ–ç»“æ„)
INDEX_PATHS = {
    "resource": "data/resource.json",  # é¡¶å±‚ç´¢å¼•
    # å­ç›®å½•ç´¢å¼•: data/resource/xxx.json
}


def mirror_head_to_access_url(url: str) -> str:
    """å°†é•œåƒæºURLè½¬æ¢ä¸ºè®¿é—®èµ„æºçš„URLæ ¼å¼"""
    if 'jsdelivr.net' in url:
        return f"{url.rstrip('/')}/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}@{GITHUB_BRANCH}"
    else:
        return f"{url.rstrip('/')}/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}"


async def test_mirror_speed(tag: str, base_url: str) -> Tuple[str, str, float, Optional[Dict]]:
    """æµ‹è¯•å•ä¸ªGitHubé•œåƒæºé€Ÿåº¦ï¼Œå¹¶å°è¯•è·å–resource.json"""
    url = mirror_head_to_access_url(base_url) + f"/{INDEX_PATHS['resource']}"
    
    async with httpx.AsyncClient() as client:
        try:
            start_time = time.time()
            response = await client.get(url, timeout=10.0)
            elapsed_time = time.time() - start_time
            
            if response.status_code == 200:
                logger.debug(f'âŒ› [æµ‹é€Ÿ] {tag} {base_url} å»¶æ—¶: {elapsed_time:.2f}s')
                # å°è¯•è§£æJSONè·å–last_updated
                try:
                    data = json.loads(response.text)
                    if "last_updated" in data:
                        return tag, base_url, elapsed_time, data
                    else:
                        logger.warning(f'âš ï¸ {tag} {base_url} JSONæ ¼å¼é”™è¯¯: ç¼ºå°‘last_updated')
                        return tag, base_url, elapsed_time, None
                except json.JSONDecodeError:
                    logger.warning(f'âš ï¸ {tag} {base_url} JSONè§£æå¤±è´¥')
                    return tag, base_url, elapsed_time, None
            else:
                logger.warning(f'âš ï¸ {tag} {base_url} æµ‹è¯•æ–‡ä»¶çŠ¶æ€ç : {response.status_code}')
        except Exception as e:
            logger.warning(f'âš ï¸ {tag} {base_url} è¿æ¥é”™è¯¯: {str(e)[:50]}...')
    
    return tag, base_url, float('inf'), None


async def check_speed():
    """æµ‹é€Ÿé€‰æ‹©æœ€å¿«çš„GitHubé•œåƒæºï¼Œæ¯”è¾ƒèµ„æºæ–°é²œåº¦"""
    global global_tag, global_url, NOW_SPEED_TEST
    
    if (not global_tag or not global_url) and not NOW_SPEED_TEST:
        NOW_SPEED_TEST = True
        logger.info('[WWèµ„æºä¸‹è½½]æµ‹é€Ÿä¸­...')
        
        # ç¬¬ä¸€æ­¥ï¼šæµ‹è¯•æ‰€æœ‰æºï¼ˆåŒ…æ‹¬ç›´è¿å’Œé•œåƒï¼‰
        tasks = []
        for tag, base_url in GITHUB_MIRRORS:
            tasks.append(asyncio.create_task(test_mirror_speed(tag, base_url)))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†å¯ç”¨çš„æº
        raw_source = None  # ç›´è¿æº
        mirror_sources = []  # é•œåƒæº
        
        for result in results:
            if isinstance(result, (Exception, BaseException)):
                continue
            tag, base_url, elapsed, json_data = result
            
            if elapsed < float('inf'):  # å¯ç”¨çš„æº
                source_info = {
                    'tag': tag,
                    'url': base_url.rstrip('/'),
                    'time': elapsed,
                    'json': json_data
                }
                
                # åˆ†ç±»
                if tag == "[GitHub Raw]":
                    raw_source = source_info
                else:
                    mirror_sources.append(source_info)
        
        # ç¬¬äºŒæ­¥ï¼šå†³ç­–é€»è¾‘
        selected_source = None
        
        if not raw_source:
            # æƒ…å†µ1: ç›´è¿ä¸å¯ç”¨ -> ä½¿ç”¨æœ€å¿«é•œåƒ
            logger.info('âŒ GitHub Rawä¸å¯ç”¨ï¼Œä½¿ç”¨æœ€å¿«é•œåƒæº')
            if mirror_sources:
                # æŒ‰é€Ÿåº¦æ’åº
                mirror_sources.sort(key=lambda x: x['time'])
                selected_source = mirror_sources[0]
        else:
            # æƒ…å†µ2: ç›´è¿å¯ç”¨
            logger.info('âœ… GitHub Rawå¯ç”¨ï¼Œå¼€å§‹æ™ºèƒ½é€‰æ‹©...')
            
            # 2.1 æ‰¾å‡ºæœ€å¿«é•œåƒæº
            fastest_mirror = None
            if mirror_sources:
                mirror_sources.sort(key=lambda x: x['time'])
                fastest_mirror = mirror_sources[0]
            
            if not fastest_mirror:
                # æ²¡æœ‰å¯ç”¨é•œåƒï¼Œä½¿ç”¨ç›´è¿
                logger.info('â„¹ï¸ æ²¡æœ‰å¯ç”¨é•œåƒæºï¼Œä½¿ç”¨ç›´è¿æº')
                global_tag = raw_source['tag']
                global_url = mirror_head_to_access_url(raw_source['url'])
                NOW_SPEED_TEST = False
                return global_tag, global_url

            logger.info(f'ğŸ” æœ€å¿«é•œåƒæº: {fastest_mirror["tag"]} å»¶æ—¶: {fastest_mirror["time"]:.2f}s')
            
            # 2.2 æ ¹æ®JSONè·å–æƒ…å†µå†³ç­–
            has_raw_json = raw_source['json'] is not None
            has_mirror_json = fastest_mirror['json'] is not None
            
            if not has_raw_json and not has_mirror_json:
                # åŒæ–¹éƒ½è·å–å¤±è´¥ï¼Œä½¿ç”¨ç›´è¿
                logger.warning('âš ï¸ åŒæ–¹JSONè·å–å¤±è´¥ï¼Œä½¿ç”¨ç›´è¿æº')
                selected_source = raw_source
            elif not has_raw_json:
                # ç›´è¿JSONè·å–å¤±è´¥ï¼Œä½¿ç”¨é•œåƒ
                logger.info('ğŸ“¥ ç›´è¿JSONè·å–å¤±è´¥ï¼Œä½¿ç”¨é•œåƒæº')
                selected_source = fastest_mirror
            elif not has_mirror_json:
                # é•œåƒJSONè·å–å¤±è´¥ï¼Œä½¿ç”¨ç›´è¿
                logger.info('ğŸ“¥ é•œåƒJSONè·å–å¤±è´¥ï¼Œä½¿ç”¨ç›´è¿æº')
                selected_source = raw_source
            else:
                # åŒæ–¹éƒ½æœ‰JSONï¼Œæ¯”è¾ƒlast_updated
                raw_updated = raw_source['json'].get('last_updated', '')
                mirror_updated = fastest_mirror['json'].get('last_updated', '')
                
                logger.debug(f'ğŸ“… ç›´è¿æ›´æ–°æ—¥æœŸ: {raw_updated} é•œåƒæ›´æ–°æ—¥æœŸ: {mirror_updated}')
                
                if mirror_updated >= raw_updated:
                    # é•œåƒç«™æ˜¯æœ€æ–°æˆ–ä¸€æ ·æ–° -> ä½¿ç”¨é•œåƒç«™
                    logger.info('ğŸ”„ é•œåƒç«™èµ„æºå·²åŒæ­¥æˆ–æ›´æ–°ï¼Œä½¿ç”¨é•œåƒç«™')
                    selected_source = fastest_mirror
                else:
                    # é•œåƒç«™è½å -> ä½¿ç”¨ç›´è¿
                    logger.info('âš¡ é•œåƒç«™èµ„æºè½åï¼Œä½¿ç”¨ç›´è¿æº')
                    selected_source = raw_source
        
        # ç¬¬ä¸‰æ­¥ï¼šè®¾ç½®å…¨å±€å˜é‡
        if selected_source:
            global_url = mirror_head_to_access_url(selected_source['url'])
            global_tag = selected_source['tag']
            logger.info(f"ğŸš€ æœ€ç»ˆé€‰æ‹©: {global_tag} {global_url}")
        else:
            # åå¤‡æ–¹æ¡ˆ
            global_url = mirror_head_to_access_url("https://raw.githubusercontent.com")
            global_tag = "[GitHub Raw]"
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åˆé€‚æºï¼Œä½¿ç”¨ç›´è¿ï¼ˆå¯èƒ½ä¸å¯ç”¨ï¼‰: {global_tag}")
        
        NOW_SPEED_TEST = False
        return global_tag, global_url
    
    if NOW_SPEED_TEST:
        while NOW_SPEED_TEST:
            await asyncio.sleep(0.5)
    
    return global_tag, global_url


async def fetch_json_index(client: httpx.AsyncClient, base_url: str, json_path: str) -> Optional[Dict]:
    """è·å–å¹¶è§£æJSONç´¢å¼•æ–‡ä»¶"""
    url = f"{base_url.rstrip('/')}/{json_path}"
    try:
        response = await client.get(url, timeout=30.0)
        if response.status_code == 200:
            return json.loads(response.text)
    except Exception as e:
        logger.warning(f"è·å–JSONç´¢å¼•å¤±è´¥ {url}: {e}")
    return None


async def download_with_json_index(
    base_url: str,
    tag: str,
    endpoint: str,
    local_path: Path,
    client: httpx.AsyncClient,
    plugin_name: str
):
    """ä½¿ç”¨JSONç´¢å¼•ä¸‹è½½å•ä¸ªç›®å½•çš„èµ„æº"""
    # ä»endpointæå–ç›®å½•å
    dir_name = endpoint.split('/')[-1] if '/' in endpoint else endpoint
    
    # è·å–ç›®å½•çš„JSONç´¢å¼•
    dir_json_path = f"data/resource/{dir_name}.json"
    dir_json = await fetch_json_index(client, base_url, dir_json_path)
    
    if not dir_json:
        logger.warning(f'{plugin_name} {tag} {endpoint} æ— æ³•è·å–JSONç´¢å¼•: {dir_json_path}')
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    files = dir_json.get("files", [])
    total_files = len(files)
    exist_files = 0
    need_download_files = 0
    logger.debug(f'{plugin_name} {tag} ç›®å½• {endpoint} ä¸­æœ‰ {total_files} ä¸ªæ–‡ä»¶å¾…æ£€æŸ¥')
    
    # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
    download_tasks = []
    size_checked = 0
    batch_size_limit = 1500000  # 1.5MB æ‰¹æ¬¡é™åˆ¶
    batch_num = 0  # æ‰¹æ¬¡ç¼–å·ï¼Œç”¨äºæ—¥å¿—
    
    for idx, file_info in enumerate(files, 1):
        file_relative_path = file_info["path"]
        remote_size = file_info.get("size", 0)
        
        # æ„å»ºæœ¬åœ°è·¯å¾„
        if file_relative_path.startswith(dir_name + "/"):
            local_relative_path = file_relative_path[len(dir_name)+1:]
        else:
            local_relative_path = file_info["name"]
        
        local_file_path = local_path / local_relative_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¤§å°ä¸€è‡´
        file_exists = local_file_path.exists()
        
        if file_exists:
            exist_files += 1
            local_size = local_file_path.stat().st_size
            
            if local_size == remote_size:
                logger.debug(f'{tag}âœ… æ–‡ä»¶å·²å­˜åœ¨: {file_relative_path}')
                continue  # æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°ä¸€è‡´ï¼Œè·³è¿‡ä¸‹è½½
            else:
                logger.info(f'{plugin_name} {tag}ğŸ”„ æ–‡ä»¶å¤§å°ä¸ä¸€è‡´: {file_relative_path} (æœ¬åœ°: {local_size}, è¿œç¨‹: {remote_size})')
        else:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        need_download_files += 1
        size_checked += remote_size
        
        # æ„å»ºä¸‹è½½URL
        file_url = f"{base_url.rstrip('/')}/data/resource/{file_relative_path}"
        
        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        task = asyncio.create_task(
            download(file_url, local_file_path.parent, local_file_path.name, client, tag)
        )
        download_tasks.append(task)
        
        # æ‰¹æ¬¡æ§åˆ¶ï¼šè¾¾åˆ°é™åˆ¶æˆ–å¤„ç†å®Œæœ€åä¸€ä¸ªæ–‡ä»¶æ—¶
        if size_checked >= batch_size_limit or idx == total_files:
            batch_num += 1
            
            if len(download_tasks) > 0:
                logger.debug(f'{tag} å¼€å§‹ç¬¬ {batch_num} æ‰¹ä¸‹è½½ï¼Œå…± {len(download_tasks)} ä¸ªæ–‡ä»¶')
                await asyncio.gather(*download_tasks)
            
            # é‡ç½®æ‰¹æ¬¡
            download_tasks.clear()
            size_checked = 0
    
    logger.info(f'{tag} ç›®å½• {endpoint} æ£€æŸ¥å®Œæˆ-> æ€»æ•°: {total_files}, æœ¬åœ°å·²å­˜åœ¨: {exist_files}, éœ€ä¸‹è½½: {need_download_files}')


async def download_all_file(
    plugin_name: str,
    EPATH_MAP: Dict[str, Path],
    URL: Optional[str] = None,
    TAG: Optional[str] = None,
):
    """ä¸»ä¸‹è½½å‡½æ•° (æ¥å£ä¿æŒä¸å˜)"""
    
    # 1. ç¡®å®šèµ„æºç«™URLå’ŒTAG
    if URL:
        TAG, BASE_URL = TAG or '[Unknown]', URL
    else:
        TAG, BASE_URL = await check_speed()
        if not BASE_URL:
            logger.error("âŒ æ— æ³•è·å–å¯ç”¨çš„èµ„æºç«™")
            return
    
    logger.info(f'ğŸ”— {plugin_name} ä½¿ç”¨èµ„æºç«™: {TAG}')
    
    # 2. è·å–é¡¶å±‚èµ„æºç´¢å¼•ï¼ŒéªŒè¯ç›®å½•å­˜åœ¨
    async with httpx.AsyncClient(timeout=httpx.Timeout(200.0)) as client:
        # è·å–é¡¶å±‚ç´¢å¼•
        resource_json = await fetch_json_index(client, BASE_URL, INDEX_PATHS['resource'])
        if not resource_json:
            logger.error(f'âŒ {plugin_name} æ— æ³•è·å–é¡¶å±‚èµ„æºç´¢å¼•ï¼Œå¯èƒ½ç´¢å¼•æ–‡ä»¶æœªç”Ÿæˆ')
            return
        
        available_dirs = resource_json.get('directories', [])
        
        # 3. éå†æ‰€æœ‰endpointè¿›è¡Œä¸‹è½½
        processed_count = 0
        for endpoint, local_path in EPATH_MAP.items():
            # æå–ç›®å½•å
            dir_name = endpoint.split('/')[-1] if '/' in endpoint else endpoint
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦åœ¨ç´¢å¼•ä¸­
            if dir_name not in available_dirs:
                logger.warning(f'âš  ç›®å½• {dir_name} ä¸åœ¨ {plugin_name} èµ„æºç´¢å¼•ä¸­ï¼Œè·³è¿‡')
                continue
            
            # ç¡®ä¿æœ¬åœ°ç›®å½•å­˜åœ¨
            local_path.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½è¯¥ç›®å½•èµ„æº
            await download_with_json_index(
                BASE_URL, TAG, endpoint, local_path, client, plugin_name
            )
            processed_count += 1
        
        # 4. æœ€ç»ˆçŠ¶æ€
        if processed_count == len(EPATH_MAP):
            logger.success(f'ğŸ± [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} æ‰€æœ‰èµ„æºå·²æ˜¯æœ€æ–°!')
        elif processed_count > 0:
            logger.success(f'ğŸ“¦ [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} å·²å®Œæˆ {processed_count}/{len(EPATH_MAP)} ä¸ªç›®å½•')
        else:
            logger.warning(f'âš  [èµ„æºæ£€æŸ¥] æ’ä»¶ {plugin_name} æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„èµ„æºç›®å½•')